# knowledge_distillation
Distilling knowledge into a smaller Llama model and smaller BERT model
